{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Recall and Precision Metrics</center>\n",
    "<font size=\"2\">\n",
    "The objective of this unit is to explain the recall and precision metrics and how to use them in Scikit-learn. \n",
    "A beginner level knowledge of Python and the Scikit-learn library will be very helpful to achieve the desired outcome from this unit.\n",
    "    \n",
    "***\n",
    "    \n",
    "##  Introduction: F1 score\n",
    "\n",
    "\n",
    "Congratulation! You are almost there.<br> \n",
    "Now that you have built your model you need to know how accurate the model is. There is no unique way for evaluating the accuracy of a model. Among several methods, F1 score is a very useful measure developed for classification models. \n",
    "Suppose that you and I work in a team and develop a classifier together. Each of us further develop different indexes for judging accuracy of the model. \n",
    "Your index measures the quantity and mine measures the quality of the predictions.\n",
    "Each index alone might be incomplete for judgment of the model but when seen together they can provide useful information about the model accuracy. To reach an agreement we can let the average of the two indexes speak out the accuracy of our model. What are the two indexes and what informaiton they provide? \n",
    "This is what we are going to explore in the next few minutes.\n",
    "\n",
    "Let's start with a simple example. Suppose that a crime has just happened in a local street where there are 100 people present in the area. The 2 criminals who commited the crime are also among these people. \n",
    "The local police just arrive and in order to find the 2 criminals, they select and interrogate 50 of the 100 people present in the area. \n",
    "Luckily, the selected 50 suspects include also the 2 criminals, and thus the police has done a perfect job in term of quantity of the findings (2 out of 2 are found). However, it took the police to hold and investigate 50 people to conclude and find the 2 real suspects, and this seems to be a low quality operation. \n",
    "The precision of the operation could have been acclaimed if the police had investigated only, say 10 people in order to find the 2 criminals (2 out of 10 instead of 2 out of 50), or it would have been perfect if only 2 people had been selected and they turned to be the 2 criminals (2 out of 2). \n",
    "\n",
    "The classification job done by the police, although very optimistic, was very good in terms of the quantity (2/2 or 100%), but poor in the terms of quality (2/50 or 4%).\n",
    "In  the context of classification and also in Scikit-learn language our quantity and quality indexes are called recall and precision, and the subsequent measure resulting from their average is called F1 score. \n",
    "We should note that due to the numerical nature of recall and precision, the F1 score is based on the harmonic average of the two rather than the common arithmetic average. In the example above the arithmetic average is about 50% whereas the harmonic average is about 7%. F1 score can be formally defined as:\n",
    "\n",
    "F1 score $= 2\\times\\frac{\\text{precision} \\times \\text{recall}}{\\text{precision}+\\text{recall}}$, \n",
    "<br>\n",
    "\n",
    "where recall $=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$,  precision $=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$, and\n",
    "\n",
    "* True Positives (TP): those predicted as positive and the truth was also positive.\n",
    "* True Negatives (TN): those predicted as negative and the truth was also negative.\n",
    "* False Positives (FP): those predicted as positive and the truth was negative.\n",
    "* False Negatives (FN): those predicted as negative and the truth was positive.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "We are done with the definiton of the recall, precision and F1 score. However, it is important to know that depending on the data as well as the objective, we should be cautious in using these measures. We explore this aspect further in the rest of the unit.\n",
    "</div>\n",
    "\n",
    "### **Summary:**\n",
    "* F1 score is the harmonic average of recall and precision.\n",
    "* recall and precision are two measures for evaluating accuracy of classifiers and complement other measures.\n",
    "* recall and precision will change in opposite direction by changing the threshold of the decision function.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font size=\"2\">\n",
    "\n",
    "### Task 1\n",
    "The goal of this task is to bring into your attention the trade-off between recall and precision. Trained classifiers use a threshold in order to decide about instances and assign them into the outcome classes and thus resulting in a certain F1 score. Howevere, changing the threshold will result in a different allocation and thus a new set of recall and precision and therefore a new F1 score. The change in the score is due to the changes in the recall and precision in opposite direction. In general, increasing precision reduces recall and vice versa. That is why we face a trade-off. Opting for a high precision or recall could depend on the objective of the classification and sometimes the dataset. In some situations we need a high precision, and in some others we mostly care about recall.</br>\n",
    "To emphasis this further, for each of the cases bellow explain if you would require a high recall or precision:\n",
    "* You train a classifier to detect videos that are safe for kids.\n",
    "* You train a classifier to detect shoplifters on surveillance images.\n",
    "* You train a classifier to detect medical diagnosis.fraudulent transactions.\n",
    "* You train a classifier to detect fraudulent transactions.\n",
    "\n",
    "\n",
    "\n",
    "### Task 2\n",
    "The goal of this task is to see together how the F1 score can be calculated in practice. We use a dataset that includes 8 measurements for 768 patients and an outcome variable that specifies if each patient has diabetes or not. \n",
    "We are going to assess the accuracy of a classifier that uses the 8 measurements and predicts whether or not a patient has diabetes. We use logistic regression for this classification but the focus of the exercise is not on the modeling part, instead we want to have a comparison across the accuracy measures with an emphasis on the F1 score.</br>\n",
    "Start by running `RunMe.py` and `get_data()` functions and then the rest of the cells. If interested you can find further details about the data [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database).  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run RunMe.py\n",
    "X_train, X_test, y_train, y_test=get_data(visual='No')\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate test sets such that all contain the same distribution of outcomes, or as close as possible\n",
    "cv = StratifiedKFold(n_splits=10, shuffle = False, random_state = 100)\n",
    "# train the regression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "# predicting for the cross validated sets\n",
    "y_pred    = cross_val_predict(LR, X_train, y_train, cv = cv, method='predict')\n",
    "y_pred_p  = cross_val_predict(LR, X_train, y_train, cv = cv, method='predict_proba')\n",
    "y_pred_p  = y_pred_p[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting lets assume that having diabetes is a positive outcome and no diabetes is a negative outcome. Thus, for instance, **True Positives** is the number of persons *truly* predicted by the model to have diabetes, and **False Negatives** is the number of persons *falsely* predicted by the model to not to have diabetes.</br>\n",
    "Now we can manually compute the recall and precision using the information from **confusion matrix**.\n",
    "<b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('confusion_matrix:', metrics.confusion_matrix(y_train, y_pred), sep ='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of the matrix shows **TN**=326 and **FP**=42, whereas the second row shows **FN**=91 and **TP**=108. Using the formula in the previous section, we can compute recall, precision and F1 score as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall $=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}=\\frac{108}{108+91}=0.54$<br>\n",
    "\n",
    "precision $=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}=\\frac{108}{108+42}=0.72$<br>\n",
    "\n",
    "F1 score $= 2\\frac{0.54 \\times 0.72}{0.54+0.72}=0.62$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn function `classification_report` also provides a report which includes these measures: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_train, y_pred, target_names=['no diabetes','diabetes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in Task 1, a trade-off exists between recall and precision, which can be illustrated using the Scikit-learn `precision_recall_curve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_pred_p)\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], label='precision')\n",
    "plt.plot(thresholds, recalls[:-1],label='recall')\n",
    "plt.xlabel('threshold')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change the default threshold, 0.5, and check the changes in the recall and precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold=0.5\n",
    "new_precision = precision_score(y_train, y_pred_p > new_threshold)\n",
    "new_recall    = recall_score(y_train, y_pred_p > new_threshold)\n",
    "print('precision: %.2f' % new_precision,'recall: %.2f' % new_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>: F1 score is not the only measure to assess the model accuracy. A simple measure used for this purpose is Classification Accuracy which can be easily obtained from the confusion matrix. Another measure is the area under the ROC curve, or AUC. Bellow is a short summary of these measures:\n",
    "\n",
    "* F1 score\n",
    "    * depends on the threshold\n",
    "    * should be used when there is a moderate to large class imbalance\n",
    "    \n",
    "\n",
    "* Accuracy\n",
    "    * depends on the threshold\n",
    "    * should be used for balanced data\n",
    "    \n",
    "    \n",
    "* AUC\n",
    "    * should be used for imbalanced data\n",
    "    * independent from the threshold because it only considers the rank of each prediction and not its absolute value\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurcy  = cross_val_score(LR, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "roc_auc  = cross_val_score(LR, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\n",
    "print('accurcy: %.2f' % accurcy,'roc_auc: %.2f' % roc_auc, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Use the same dataset and do the followings:\n",
    "* Check how balanced the data is and then decide which accuracy measure can be used.\n",
    "* Try scaling the measurements and see if the accuracy measures will improve.\n",
    "* Change the penalty function in the Logistic Regression to $\\ell_1$ which is less sensitive to outliers and  see if the accuracy measures will improve.\n",
    "* Develop a classifier of your choice (e.g. `RandomForestClassifier`) and apply what you have learned so far in this unit. In particular, compare the 'accurcies' of the two classifiers. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>: imbalanced data refers to the situation where one of the classes holds most of the sample. For instance in our dataset we could have only 5% of the patient to have diabetes.\n",
    "   \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
